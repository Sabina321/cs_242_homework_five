\documentclass{article}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage[margin=1in]{geometry}
\setlength\parindent{0pt}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}
% \usepackage{subcaption}
%\usepackage{tabularx}
%\usepackage{float}

\begin{document}


\begin{center}
\Huge{\textsc{Homework Five Proposal}} 

\Large\textsc{CMPS242 - Fall 2015}\\

\large{Guangyu Wang \hfill Andrei Ignat  \hfill Sabina Tomkins} 
\end{center}

\section*{Preprocessing}
Before we begin with any sort of prediction for the \textbf{Titanic} dataset, we must first preprocess the data. For this we want to turn the \texttt{sex}, \texttt{age}, \texttt{sibsp}, \texttt{parch}, and \texttt{embarked} variables into numerical variables by using some sort of numerical encoding scheme. The \texttt{name} and \texttt{ticket} variables do not bear any relevant information for our model since each data point is already labeled by a unique id.

% What do we do with the cabin? It's there but not for everyone

\section*{Model}
For the purpose of prediction, we would like to build an \textbf{Ensemble Learning} model. As part of this model, we will include the \textit{Random Forrest Classifier} since this is the benchmark used by the authors of the sample script. Furthermore, we would like to integrate the following techniques in the ensemble and namely:

\begin{itemize}
\item \textbf{Logistic Regression} - this is an all time favorite in ML competitions and for good reason. We want a model which can learn how to predict binary features, representing whether or not the passenger survived. Logistic regression can handle outliers(which are to be expected in a dataset this small) and is interpretable.

\item \textbf{Support Vector Machines} - the good part of the support vector machine is that it is definitely a good model for predicting binary labels. However, we expect that finding the right kernel for the dataset might prove a bit difficult. Nonetheless, it's worth a try.

\item \textbf{Neural Networks} - training a neural network on the dataset also seems to be a promising idea. The features and their potential internal interactions make the dataset suitable for a neural network approach.

%\item \textbf{Probabilistic Soft Logic}

\item \textbf{Deep Neural Networks} - the last method in the ensemble is also a bit of a reach method. Nonetheless, given the recent successes of deep neural networks, it would almost be a shame not to try and see how they fare against the other methods in the ensemble.
\end{itemize}

%I'm bullshitting over here
In order to combine all the results, we would like to use \textbf{AdaBoost} and then output a boosted prediction.
\end{document}
