\documentclass{article}
\usepackage{amsmath}
\usepackage{verbatim}
%\usepackage[margin=0.5in]{geometry}
\setlength\parindent{0pt}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}
% \usepackage{subcaption}
%\usepackage{tabularx}
%\usepackage{float}

\begin{document}


\begin{center}
\Huge{\textsc{Homework 3}} 
\Large\textsc{CMPS242 - Fall 2015}\\

\large{Ehsan Amid \hfill Andrei Ignat  \hfill Sabina Tomkins} 
\end{center}

\section{Problem 1}

a) After saving the file in an aff file, we run the linear regression algorithm on the data and get the following values for the parameters:

$w = [-0.1343, 1.8477,  -0.8966]^\top$ and $b = 4.3608$ where $\hat{y} = w^\top \mathbf{x} + b$

The root mean squared error on the training set is: $0.1897$\newline

b) The prediction for the new instance $\mathbf{x} = [3, 3, 5]$: $\hat{t} = 5.0180$\newline

c) Now, we set the regularizer $\lambda = 0.2$ and run the first two parts again to get:

$w = [-0.1527, 2.0598, -0.6439]^\top$ and $b = 1.9483$

As can be seen, the weights become smaller, compared to (a), as we expected.

The root mean squared error on the training set becomes: $0.4614$\newline 

d) By doing the calculations by hand, we find the following weights:

$w = [ -0.1343, 1.8477, -0.8966]^\top$ and $b = 4.3608$ and error $= 0.1897$, which is almost the same as part (a), because Weka uses a very small regularizer parameter by default ($1e-8$).\newline

e) The least squared error solution does not depend on the order of the examples because what we are trying to minimize is the sum of the squared errors.\newline

\section{Problem Two}
\subsection{Part a}
Select the Use training set test option and run the three classifiers. Report their results (accuracies). Which algorithm is best and why?

\begin{table}[h]
    \begin{center}
    \begin{tabular}{|c|c|c|c|}
   \hline
        & Nearest Neighbor & Naive Bayes & Logistic Regression \\ \hline
         Accuracy Training Set &  100\%&76.3021\% & 78.2552\%   \\ \hline
         Accuracy 66\% Split &  72.7969 \% &77.0115 \% & 80.0766 \% \\\hline
             
     
    \end{tabular}
    \end{center}
\end{table}

At first glance it may appear that 1 Nearest Neighbor is the best. However it's performance is so nice on the training set that one is agitated by a fear of the model having overfitted the data. Indeed that seems to be the case, when we perform a 66\% split training, testing, we see that the 1 Nearest Neighbor is the least impressive of the lot. 

\subsection{Part b}

\section{Problem 4}

Note that

\begin{equation*}
\frac{d \sigma(a)}{d a} = \frac{\exp(-a)}{(1+ \exp(-a))^2} = \sigma(-a) (1-\sigma(-a))
\end{equation*}
and
\begin{equation*}
\frac{d y_n}{d \mathbf{w}} = \sigma(-a) (1-\sigma(-a)) \mathbf{x}_n = y_n (1-y_n) \mathbf{x}_n
\end{equation*}
Taking the derivative of $E(\mathbf{w})$ w.r.t. $\mathbf{w}$ and substituting for these $\frac{d y_n}{d \mathbf{w}}$ from above, we have:
\begin{equation*}
\nabla E(\mathbf{w}) = \sum_{n=1}^N (\frac{t_n}{y_n}  - \frac{(1-t_n)}{(1-y_n)}) \frac{d y_n}{d \mathbf{w}} = \sum_{n=1}^N (t_n(1-y_n)-(1-t_n)y_n) \mathbf{x}_n = \sum_{n=1}^N(t_n - y_n)\mathbf{x}_n
\end{equation*} 

\end{document}
